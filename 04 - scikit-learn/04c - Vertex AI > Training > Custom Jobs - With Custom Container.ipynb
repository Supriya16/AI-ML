{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04c - Vertex AI > Training > Custom Jobs - With Custom Container\n",
    "\n",
    "*This notebook was developed in collaboration with [statmike](https://github.com/statmike), basing much of the core code from existing notebooks and extending to new features. Notebook author: [goodrules](https://github.com/goodrules)*\n",
    "\n",
    "### 04 Series Overview\n",
    "Where a model gets trained is where it consumes computing resources.  With Vertex AI, you have choices for configuring the computing resources available at training.  This notebook is an example of an execution environment.  When it was set up there were choices for machine type and accelerators (GPUs).  \n",
    "\n",
    "In the `04` notebook, the model training happened directly in the notebook.  The models were then imported to Vertex AI and deployed to an endpoint for online predictions. \n",
    "\n",
    "In this `04a-04i` series of demonstrations, the same model is trained using managed computing resources in Vertex AI as custom training jobs.  These jobs will be demonstrated as:\n",
    "\n",
    "-  Custom Job from a python script (`04a`), python source distribution (`04b`), and custom container (`04c`)\n",
    "-  Training Pipeline that trains and saves models from a python script (`04d`), python source distribution (`04e`), and custom container (`04f`)\n",
    "-  Hyperparameter Tuning Jobs from a python script (`04g`), python source distribution (`04h`), and custom container (`04i`)\n",
    "\n",
    "### This Notebook (`04c`): An extension of `04a` that saves the Python script in a custom container. \n",
    "This notebook trains the same Tensorflow Keras model from `04` by first modifying and saving the training code as a Python module on a custom container.  While this example fits nicely in a single script, larger examples will benefit from the flexibility offered by source distributions or module storage and this notebook gives an example of making the shift.   The custom container approach will also offer faster startup times and more flexbility in areas like frequency of updates, choice of frame works, and distributed training options.\n",
    "\n",
    "The training code is stored directly on the custom container as part of the Docker build process.  This build process uses a pre-built container as the base image and adds both packages and the training code as a Python module.  The training is conducted as a Vertex AI > Training > Custom Job that is also assigned compute resources along with the custom container for executing the training in a managed service.  This is done with the [Vertex AI Python SDK](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#) using the class [`aiplatform.CustomJob()`](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomJob).\n",
    "\n",
    "<img src=\"architectures/overview/Training.png\">\n",
    "\n",
    "### Prerequisites:\n",
    "-  01 - BigQuery - Table Data Source\n",
    "-  Understanding:\n",
    "    -  04 - Vertex AI > Notebooks - Models Built in Notebooks with sklearn\n",
    "        -  Contains a more granular review of the sklearn model training\n",
    "\n",
    "### Overview:\n",
    "- Setup Environment\n",
    "- Training\n",
    "    - Assemble a Python file/script for training\n",
    "    - Create a Custom Container containing the Python Script\n",
    "    - Store the Custom Container in Artifact Registry\n",
    "    - Setup the Vertex AI > Training > Custom Job\n",
    "    - Run the Vertex AI > Training > Custom Job\n",
    "- Serving\n",
    "    - Upload The Model to Vertex AI > Models\n",
    "    - Create an Endpoint with Vertex AI > Endpoints\n",
    "    - Deploy the Model to the Endpoint\n",
    "- Prediction\n",
    "    - Prepare a record for prediction\n",
    "    - Get Predictions with Python Client\n",
    "    - Get Predictions with REST\n",
    "    - Get Prediction with gcloud CLI\n",
    "\n",
    "### Resources:\n",
    "- [Vertex AI Custom Container For Training](https://cloud.google.com/vertex-ai/docs/training/containers-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI - Conceptual Flow\n",
    "\n",
    "<img src=\"architectures/slides/04c_arch.png\">\n",
    "\n",
    "---\n",
    "## Vertex AI - Workflow\n",
    "\n",
    "<img src=\"architectures/slides/04c_console.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mg-ce-demos'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = '04c'\n",
    "\n",
    "# Resources\n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/sklearn-cpu.0-23'\n",
    "DEPLOY_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest'\n",
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest'\n",
    "SKLEARN_VERSION = '0.23'\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id Class splits'\n",
    "C = '1.0' # based on 'best' model from notebook 04\n",
    "SOLVER = 'newton-cg' # based on 'best' model from notebook 04\n",
    "PENALTY = 'l2' # based on 'best' model from notebook 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bigquery = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "BLOB = f\"{DATANAME}/models/{NOTEBOOK}/{TIMESTAMP}/model/model.joblib\"\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mg-ce-demos-main@mg-ce-demos.iam.gserviceaccount.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give service account roles/storage.objectAdmin permissions\n",
    "# Console > IMA > Select Account <projectnumber>-compute@developer.gserviceaccount.com > edit - give role\n",
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Python File for Training\n",
    "\n",
    "Create the main python trainer file as `/train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DIR}/source/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp/04c/source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/trainer/train.py\n",
    "\n",
    "# package import\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--c_value', dest = 'c_value', default = 1.0, type = float, help = 'C value')\n",
    "parser.add_argument('--penalty', dest = 'penalty', default = 'l2', type = str, help = 'Penalty term')\n",
    "parser.add_argument('--solver', dest = 'solver', default = 'newton-cg', type = str, help = 'Logistic regression solver')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str)\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--dataname', dest = 'dataname', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--notebook', dest = 'notebook', type=str)\n",
    "parser.add_argument('--bucket', dest = 'bucket', type=str)\n",
    "parser.add_argument('--blob', dest = 'blob', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# built in parameters for data source:\n",
    "PROJECT_ID = args.project_id\n",
    "DATANAME = args.dataname\n",
    "REGION = args.region\n",
    "NOTEBOOK = args.notebook\n",
    "BUCKET = args.bucket\n",
    "BLOB = args.blob\n",
    "VAR_OMIT = str(args.var_omit).split(' ')\n",
    "\n",
    "# clients\n",
    "bigquery = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {DATANAME}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{DATANAME}_prepped'\"\n",
    "schema = bigquery.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bigquery.query(query = f'SELECT DISTINCT {args.var_target} FROM {DATANAME}.{DATANAME}_prepped WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "\n",
    "# read datasets from BigQuery\n",
    "train_query = f\"SELECT * FROM {DATANAME}.{DATANAME}_prepped WHERE splits = 'TRAIN'\"\n",
    "train = bigquery.query(train_query).to_dataframe()\n",
    "X_train = train.loc[:, ~train.columns.isin(VAR_OMIT)]\n",
    "y_train = train[args.var_target]\n",
    "\n",
    "# Logistic Regression\n",
    "clf = LogisticRegression(C=args.c_value, penalty=args.penalty, solver=args.solver, random_state=18) #optimal hyperparameters from GridSearchCV - see notebook 04\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Output model file\n",
    "joblib.dump(model, 'model.joblib') # the model needs to be named model.joblib is order for model upload to be successful\n",
    "\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(BUCKET)\n",
    "blob = bucket.blob(BLOB)\n",
    "blob.upload_from_filename('model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Container\n",
    "- https://cloud.google.com/vertex-ai/docs/training/create-custom-container\n",
    "- https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "- https://cloud.google.com/vertex-ai/docs/general/deep-learning\n",
    "    - https://cloud.google.com/deep-learning-containers/docs/choosing-container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Base Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/deeplearning-platform-release/sklearn-cpu.0-23'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE # Defined above in Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "A basic dockerfile thats take the base image and copies the code in and define an entrypoint - what python script to run first in this case.  Add RUN entries to pip install additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {BASE_IMAGE}\n",
    "WORKDIR /\n",
    "## Copies the trainer code to the docker image\n",
    "COPY trainer /trainer\n",
    "## Sets up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n",
    "\"\"\"\n",
    "with open(f'{DIR}/source/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Artifact Registry\n",
    "\n",
    "The container will need to be stored in Artifact Registry, Container Registry or Docker Hub in order to be used by Vertex AI Training jobs.  This notebook will setup Artifact registry and push a local (to this notebook) built container to it. \n",
    "\n",
    "https://cloud.google.com/artifact-registry/docs/docker/store-docker-container-images#gcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enable Artifact Registry API:\n",
    "Check to see if the api is enabled, if not then enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Registry is Enabled for This Project: mg-ce-demos\n"
     ]
    }
   ],
   "source": [
    "services = !gcloud services list --format=\"json\" --available --filter=name:artifactregistry.googleapis.com\n",
    "services = json.loads(\"\".join(services))\n",
    "\n",
    "if (services[0]['config']['name'] == 'artifactregistry.googleapis.com') & (services[0]['state'] == 'ENABLED'):\n",
    "    print(f\"Artifact Registry is Enabled for This Project: {PROJECT_ID}\")\n",
    "else:\n",
    "    print(f\"Enabeling Artifact Registry for this Project: {PROJECT_ID}\")\n",
    "    !gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create A Repository\n",
    "Check to see if the registry is already created, if not then create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a repository named mg-ce-demos\n"
     ]
    }
   ],
   "source": [
    "check_for_repo = !gcloud artifacts repositories describe {PROJECT_ID} --location={REGION}\n",
    "\n",
    "if check_for_repo[0].startswith('ERROR'):\n",
    "    print(f'Creating a repository named {PROJECT_ID}')\n",
    "    !gcloud  artifacts repositories create {PROJECT_ID} --repository-format=docker --location={REGION} --description=\"Vertex AI Training Custom Containers\"\n",
    "else:\n",
    "    print(f'There is already a repository named {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure Local Docker to Use GCLOUD CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/Users/mikegoodman/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Custom Container (local to notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/mg-ce-demos/mg-ce-demos/04c_fraud:latest'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PROJECT_ID}/{NOTEBOOK}_{DATANAME}:latest\"\n",
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 280B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for gcr.io/deeplearning-platform-release/skl  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 280B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for gcr.io/deeplearning-platform-release/skl  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 280B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for gcr.io/deeplearning-platform-release/skl  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (5/6)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 280B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for gcr.io/deeplearning-platform-release/skl  0.4s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.93kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM gcr.io/deeplearning-platform-release/sklearn-cpu.0-23@sha2  0.0s\n",
      "\u001b[0m => CACHED [2/3] COPY trainer /trainer                                     0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (7/7) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 280B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for gcr.io/deeplearning-platform-release/skl  0.4s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.93kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM gcr.io/deeplearning-platform-release/sklearn-cpu.0-23@sha2  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] COPY trainer /trainer                                     0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:9466d736e3dc35e87cebc1b9e846a294711cfe4fe4374  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to us-central1-docker.pkg.dev/mg-ce-demos/mg-ce-demos/04c_f  0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "!docker build {DIR}/source/. -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test The Custom Container (local to notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/trainer/train.py\", line 44, in <module>\n",
      "    bigquery = bigquery.Client(project = PROJECT_ID)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/client.py\", line 235, in __init__\n",
      "    _http=_http,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/google/cloud/client/__init__.py\", line 322, in __init__\n",
      "    self, credentials=credentials, client_options=client_options, _http=_http\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/google/cloud/client/__init__.py\", line 178, in __init__\n",
      "    credentials, _ = google.auth.default(scopes=scopes)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/google/auth/_default.py\", line 616, in default\n",
      "    raise exceptions.DefaultCredentialsError(_HELP_MESSAGE)\n",
      "google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started\n"
     ]
    }
   ],
   "source": [
    "#!docker run {IMAGE_URI} --project_id {PROJECT_ID} --dataname {DATANAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push The Custom Container To Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/mg-ce-demos/mg-ce-demos/04c_fraud]\n",
      "\n",
      "\u001b[1B9b6693d4: Preparing \n",
      "\u001b[1B13fc1c23: Preparing \n",
      "\u001b[1B034092dc: Preparing \n",
      "\u001b[1B62ed2ffb: Preparing \n",
      "\u001b[1B7c522b7d: Preparing \n",
      "\u001b[1B7c6e108e: Preparing \n",
      "\u001b[1B0fe60cdc: Preparing \n",
      "\u001b[1Beba7f5a3: Preparing \n",
      "\u001b[1B9ad9cb4d: Preparing \n",
      "\u001b[4B0fe60cdc: Waiting g \n",
      "\u001b[1B7c9cb170: Preparing \n",
      "\u001b[3Bd997fdeb: Waiting g \n",
      "\u001b[3B7c9cb170: Waiting g \n",
      "\u001b[1B36c1940e: Preparing \n",
      "\u001b[1B4e5c22e4: Preparing \n",
      "\u001b[2B4e5c22e4: Waiting g \n",
      "\u001b[2Bca552b47: Waiting g \n",
      "\u001b[1Bbf18a086: Waiting g \n",
      "\u001b[1B09a492af: Preparing \n",
      "\u001b[3Bbf18a086: Preparing \n",
      "\u001b[1B834cbd4e: Preparing \n",
      "\u001b[1B9ac077d9: Preparing \n",
      "\u001b[1B8156a203: Layer already exists \u001b[17A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2Klatest: digest: sha256:8b3d419541c563a12de57d88eddaf570bdcf6c528a4cb591d1c6510a47330317 size: 5335\n"
     ]
    }
   ],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--c_value=\" + C,\n",
    "    \"--penalty=\" + PENALTY,\n",
    "    \"--solver=\" + SOLVER,\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--dataname=\" + DATANAME,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--notebook=\" + NOTEBOOK,\n",
    "    \"--bucket=\" + BUCKET,\n",
    "    \"--blob=\" + BLOB\n",
    "]\n",
    "\n",
    "MACHINE_SPEC = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_count\": 0\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": MACHINE_SPEC,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": f\"{IMAGE_URI}\",\n",
    "            \"command\": [],\n",
    "            \"args\": CMDARGS\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    worker_pool_specs = WORKER_POOL_SPEC,\n",
    "    base_output_dir = f\"{URI}/{TIMESTAMP}\",\n",
    "    staging_bucket = f\"{URI}/{TIMESTAMP}\",\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/633472233130/locations/us-central1/customJobs/231810861117734912\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/633472233130/locations/us-central1/customJobs/231810861117734912')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/231810861117734912?project=633472233130\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/633472233130/locations/us-central1/customJobs/231810861117734912 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/633472233130/locations/us-central1/customJobs/231810861117734912\n"
     ]
    }
   ],
   "source": [
    "customJob.run(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04c_fraud_20221024092349'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customJob.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new model, creating in model registry\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/633472233130/locations/us-central1/models/model_04c_fraud/operations/921374594999255040\n",
      "Model created. Resource name: projects/633472233130/locations/us-central1/models/972280540256272384@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/633472233130/locations/us-central1/models/972280540256272384@1')\n"
     ]
    }
   ],
   "source": [
    "modelmatch = aiplatform.Model.list(filter = f'display_name=\"{NOTEBOOK}_{DATANAME}\"')\n",
    "if modelmatch:\n",
    "    print(\"Model Already in Registry:\")\n",
    "    if f'time-{TIMESTAMP}' in modelmatch[0].version_aliases:\n",
    "        print(\"This version already loaded, no action taken.\")\n",
    "        model = aiplatform.Model(model_name = modelmatch[0].resource_name)\n",
    "    else:\n",
    "        print('Loading model as new default version.')\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name = f'{NOTEBOOK}_{DATANAME}',\n",
    "            model_id = f'model_{NOTEBOOK}_{DATANAME}',\n",
    "            parent_model =  modelmatch[0].resource_name,\n",
    "            serving_container_image_uri = DEPLOY_IMAGE,\n",
    "            artifact_uri = f\"{URI}/{TIMESTAMP}/model\",\n",
    "            is_default_version = True,\n",
    "            version_aliases = [f'time-{TIMESTAMP}'],\n",
    "            version_description = f'time-{TIMESTAMP}',\n",
    "            labels = {'notebook':f'{NOTEBOOK}'}        \n",
    "        )\n",
    "else:\n",
    "    print('This is a new model, creating in model registry')\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name = f'{NOTEBOOK}_{DATANAME}',\n",
    "        model_id = f'model_{NOTEBOOK}_{DATANAME}',\n",
    "        serving_container_image_uri = DEPLOY_IMAGE,\n",
    "        artifact_uri = f\"{URI}/{TIMESTAMP}/model\",\n",
    "        is_default_version = True,\n",
    "        version_aliases = [f'time-{TIMESTAMP}'],\n",
    "        version_description = f'time-{TIMESTAMP}',\n",
    "        labels = {'notebook':f'{NOTEBOOK}'}\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** on Version Aliases:\n",
    ">Expectation is a name starting with `a-z` that can include `[a-zA-Z0-9-]`\n",
    "\n",
    "**Retrieve a Model Resource**\n",
    "\n",
    "[Resource](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model)\n",
    "```Python\n",
    "model = aiplatform.Model(model_name = f'model_{NOTEBOOK}_{DATANAME}') # retrieves default version\n",
    "model = aiplatform.Model(model_name = f'model_{NOTEBOOK}_{DATANAME}@time-{TIMESTAMP}') # retrieves specific version\n",
    "model = aiplatform.Model(model_name = f'model_{NOTEBOOK}_{DATANAME}', version = f'time-{TIMESTAMP}') # retrieves specific version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create An Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Exists: projects/633472233130/locations/us-central1/endpoints/8704041908730593280\n"
     ]
    }
   ],
   "source": [
    "SERIES = ''.join(filter(str.isdigit, NOTEBOOK))\n",
    "endpoints = aiplatform.Endpoint.list(filter = f\"display_name={SERIES}_{DATANAME}\")\n",
    "if endpoints:\n",
    "    endpoint = endpoints[0]\n",
    "    print(f\"Endpoint Exists: {endpoints[0].resource_name}\")\n",
    "else:\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name = f\"{SERIES}_{DATANAME}\",\n",
    "        labels = {'notebook':f\"{SERIES}\"}    \n",
    "    )\n",
    "    print(f\"Endpoint Created: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04_fraud'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model To Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model with 100% of traffic...\n",
      "Deploying Model projects/633472233130/locations/us-central1/models/972280540256272384 to Endpoint : projects/633472233130/locations/us-central1/endpoints/8704041908730593280\n",
      "Deploy Endpoint model backing LRO: projects/633472233130/locations/us-central1/endpoints/8704041908730593280/operations/4856394769414225920\n",
      "Endpoint model deployed. Resource name: projects/633472233130/locations/us-central1/endpoints/8704041908730593280\n"
     ]
    }
   ],
   "source": [
    "dmodels = endpoint.list_models()\n",
    "\n",
    "check = 0\n",
    "if dmodels:\n",
    "    for dmodel in dmodels:\n",
    "        if dmodel.model == model.resource_name and dmodel.model_version_id == model.version_id and dmodel.id in endpoint.traffic_split:\n",
    "            print(f'This model (and version) already deployed with {endpoint.traffic_split[dmodel.id]}% of traffic')\n",
    "            check = 1\n",
    "    \n",
    "if check == 0:\n",
    "    print(f'Deploying model with 100% of traffic...')\n",
    "    endpoint.deploy(\n",
    "        model = model,\n",
    "        deployed_model_display_name = f'{NOTEBOOK}_{DATANAME}',\n",
    "        traffic_percentage = 100,\n",
    "        machine_type = DEPLOY_COMPUTE,\n",
    "        min_replica_count = 1,\n",
    "        max_replica_count = 1\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Deployed Models without Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dmodel in endpoint.list_models():\n",
    "    if dmodel.id in endpoint.traffic_split:\n",
    "        print(f\"Model {dmodel.display_name} has traffic = {endpoint.traffic_split[dmodel.id]}\")\n",
    "    else:\n",
    "        endpoint.undeploy(deployed_model_id = dmodel.id)\n",
    "        print(f\"Undeployed {dmodel.display_name} version {dmodel.model_version_id} as it has no traffic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a record for prediction: instance and parameters lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15689</td>\n",
       "      <td>-0.745436</td>\n",
       "      <td>1.208740</td>\n",
       "      <td>2.418708</td>\n",
       "      <td>3.033260</td>\n",
       "      <td>-0.501836</td>\n",
       "      <td>1.497792</td>\n",
       "      <td>-0.848100</td>\n",
       "      <td>0.999515</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149698</td>\n",
       "      <td>-0.066289</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>-0.088377</td>\n",
       "      <td>-0.395340</td>\n",
       "      <td>-0.422684</td>\n",
       "      <td>0.188265</td>\n",
       "      <td>0.020712</td>\n",
       "      <td>0.045274</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51457</td>\n",
       "      <td>-5.194546</td>\n",
       "      <td>-1.741200</td>\n",
       "      <td>2.409787</td>\n",
       "      <td>3.979815</td>\n",
       "      <td>4.745377</td>\n",
       "      <td>-0.470931</td>\n",
       "      <td>-6.630054</td>\n",
       "      <td>-1.774991</td>\n",
       "      <td>0.541608</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.728441</td>\n",
       "      <td>-1.909447</td>\n",
       "      <td>0.063879</td>\n",
       "      <td>-5.298245</td>\n",
       "      <td>-0.242950</td>\n",
       "      <td>-1.611578</td>\n",
       "      <td>0.186398</td>\n",
       "      <td>0.421281</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152527</td>\n",
       "      <td>2.019771</td>\n",
       "      <td>0.058431</td>\n",
       "      <td>-1.041463</td>\n",
       "      <td>1.626049</td>\n",
       "      <td>0.227237</td>\n",
       "      <td>-0.196559</td>\n",
       "      <td>-0.130095</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>-0.944086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186591</td>\n",
       "      <td>0.191799</td>\n",
       "      <td>0.445460</td>\n",
       "      <td>0.204942</td>\n",
       "      <td>0.859635</td>\n",
       "      <td>-0.411058</td>\n",
       "      <td>2.328281</td>\n",
       "      <td>-0.226044</td>\n",
       "      <td>-0.090710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134025</td>\n",
       "      <td>2.101359</td>\n",
       "      <td>-0.885248</td>\n",
       "      <td>-0.019706</td>\n",
       "      <td>-0.810424</td>\n",
       "      <td>-1.429294</td>\n",
       "      <td>-0.832277</td>\n",
       "      <td>-1.110466</td>\n",
       "      <td>-0.093680</td>\n",
       "      <td>-0.347834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.493235</td>\n",
       "      <td>1.524276</td>\n",
       "      <td>0.188728</td>\n",
       "      <td>0.619306</td>\n",
       "      <td>-0.262769</td>\n",
       "      <td>-0.123342</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>-0.048398</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161647</td>\n",
       "      <td>-0.484457</td>\n",
       "      <td>1.664356</td>\n",
       "      <td>1.947575</td>\n",
       "      <td>4.371354</td>\n",
       "      <td>0.613689</td>\n",
       "      <td>0.689506</td>\n",
       "      <td>0.536490</td>\n",
       "      <td>-0.021216</td>\n",
       "      <td>-2.204157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242229</td>\n",
       "      <td>0.374826</td>\n",
       "      <td>1.234508</td>\n",
       "      <td>-0.614699</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.583962</td>\n",
       "      <td>0.825513</td>\n",
       "      <td>-0.023025</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   15689 -0.745436  1.208740  2.418708  3.033260 -0.501836  1.497792   \n",
       "1   51457 -5.194546 -1.741200  2.409787  3.979815  4.745377 -0.470931   \n",
       "2  152527  2.019771  0.058431 -1.041463  1.626049  0.227237 -0.196559   \n",
       "3  134025  2.101359 -0.885248 -0.019706 -0.810424 -1.429294 -0.832277   \n",
       "4  161647 -0.484457  1.664356  1.947575  4.371354  0.613689  0.689506   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0 -0.848100  0.999515  0.376560  ... -0.149698 -0.066289  0.035959 -0.088377   \n",
       "1 -6.630054 -1.774991  0.541608  ... -1.728441 -1.909447  0.063879 -5.298245   \n",
       "2 -0.130095 -0.015467 -0.944086  ... -0.186591  0.191799  0.445460  0.204942   \n",
       "3 -1.110466 -0.093680 -0.347834  ...  0.022980  0.493235  1.524276  0.188728   \n",
       "4  0.536490 -0.021216 -2.204157  ...  0.242229  0.374826  1.234508 -0.614699   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0 -0.395340 -0.422684  0.188265  0.020712  0.045274     0.0  \n",
       "1 -0.242950 -1.611578  0.186398  0.421281  0.010040     0.0  \n",
       "2  0.859635 -0.411058  2.328281 -0.226044 -0.090710     0.0  \n",
       "3  0.619306 -0.262769 -0.123342  0.023263 -0.048398     0.0  \n",
       "4  0.000011  0.583962  0.825513 -0.023025  0.073964     0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = f\"SELECT * FROM {DATANAME}.{DATANAME}_prepped WHERE splits = 'TEST' LIMIT 10\"\n",
    "test = bigquery.query(test_query).to_dataframe()\n",
    "X_test = test.loc[:, ~test.columns.isin(str(VAR_OMIT).split(' '))]\n",
    "y_test = test[VAR_TARGET]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [X_test.to_dict(orient='split')['data'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0], deployed_model_id='8787283735046258688', model_version_id='1', model_resource_name='projects/633472233130/locations/us-central1/models/model_04c_fraud', explanations=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances=instances)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": instances}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    0\n",
      "  ],\n",
      "  \"deployedModelId\": \"8787283735046258688\",\n",
      "  \"model\": \"projects/633472233130/locations/us-central1/models/model_04c_fraud\",\n",
      "  \"modelDisplayName\": \"04c_fraud\",\n",
      "  \"modelVersionId\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: gcloud (CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai endpoints predict {endpoint.name.rsplit('/',1)[-1]} --region={REGION} --json-request={DIR}/request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remove Resources\n",
    "see notebook \"99 - Cleanup\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
